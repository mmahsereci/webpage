<div class="pub-img"><img src="./../assets/img/publications/Siems21.jpg"></div>

**Dynamic Pruning of a Neural Network via Gradient Signal-to-Noise Ratio**<br>
While training highly overparameterized neural networks is common practice in deep learning,
research into post-hoc weight-pruning suggests that most parameters can be removed
without loss in predictive performance. In the paper, we investigate dynamic sparsity pruning that adapts the
sparse structure of the neural network during training.
We use statistics collected by the optimizer and prune weights according to their magnitude and their gradient signal-to-noise ratio.
This means that weights get pruned that are small as well as converged indicated by their gradients resembling white noise.
The procedure also yields an automated sparsity schedule.
<br>
*J. Siems, A. Klein, C. Archambeau and Maren Mahsereci* 8th Workshop on Automated Machine Learning (AutoML) @ ICML 2020.

<div class="pub-ul"><ul>
    <li><a class="button-pub" href="https://openreview.net/pdf?id=34awaeWZgya"><p>Paper</p></a></li>
    <li><a class="button-pub" href="https://slideslive.com/38962446/dynamic-pruning-of-a-neural-network-via-gradient-signaltonoise-ratio?ref=speaker-37497-latest"><p>Slides</p></a></li>
    <li><a class="button-pub" onclick="CollapseBibTeX('Siems21')"><p>BibTeX</p></a></li>
</ul></div>

<div class="pub-bib" id="Siems21">
  <blockquote>
    <div class="div-name">@inproceedings{Siems21,
        <div class="div-info">
            title={Dynamic Pruning of a Neural Network via Gradient Signal-to-Noise Ratio},<br>
            author={Julien Niklas Siems and Aaron Klein and Cedric Archambeau and Maren Mahsereci},<br>
            booktitle={8th ICML Workshop on Automated Machine Learning (AutoML) },<br>
            year={2021}
        </div>
      }
   </div>
  </blockquote>
</div>
