---
layout:     post
title:      "The Standard Error"
author:     mmahsereci
snippet:    "The law of large numbers (next post) can be combined with the standard error (SE) in order to not only get an estimate of a parameter, but also a notation of robustness of the estimate. The standard error lets us know how confident we can be about the estimate."
date:       2021-08-31
thumbnail-small:  "/img/2021-08-31-std/thumbnail.png"
category:   techblog
tags:       [Statistics, Blog]

---

The law of large numbers (previous post) can be combined with the standard error (SE) in order to
not only get an estimate of a parameter, but also a notion of the robustness of said estimate. Thus, the standard error
lets us know how confident we can be about our estimation.


In the previous post, we learned about the law of large numbers (LLM). It states that the sample mean 
$$\bar{x}_n$$ tends to the population mean $$\mu$$ in some sense for large enough $$n$$ 
(see previous post for setting and notation). 
We have also empirically observed (for fair coin tosses with $$\mu=0.5$$) that the sample size $$n$$ needs not 
be overly large (roughly $$> 100$$) in order to yield a somewhat relivable estimate of the coin-flip-parameter $$\mu$$.

Similarly, we have observed that the statistic $$\bar{x}_n$$ by definition is a random number, as it is the average of
random coin tosses. For any finite $$n$$ the statistic $$\bar{x}_n$$ thus exhibits a certain variability which means 
that it's value might be different if we repeat the experiment and toss the coin another $$n$$ times 
(and then compute $$\bar{x}_n$$ from the new coin tosses/ the new sample). 
The standard error $$\sigma_n$$ quantifies this variability. In particular the SE has the following form

$$
SE[\bar{x}_n] = \frac{\sigma}{\sqrt{n}} := \sigma_n,
$$

where $$\sigma := \operatorname{Std}[x]$$ is the standard deviation of the random variable $$x$$ that represents 
a single coin toss. It is apparent that the SE drops proportional to one over the squareroot of $$n$$, that is
$$\sigma_n\propto n^{-\frac{1}{2}}$$.
Let's have a closer look at the formula, and introduce some notation.

### The Standard Error Formula

Let $$\zeta_1, \dots, \zeta_k$$ be uncorrelated (not necessarily independent or identically 
distributed) random variables, that is 
$$\operatorname{Cov}[\zeta_j, \zeta_l] = 0$$ if $$j\neq l$$ and $$j, l=1,\dots, l$$ 
with corresponding variances $$\sigma^2_{\zeta_j}:=\operatorname{Var}[\zeta_j]$$, $$j=1,\dots, k$$, 
then Bienaym&eacute;'s formula states that the variance of the sum $$S_k^{\zeta} :=\sum_{j=1}^k \zeta_j$$ is equal to the 
sum of the variances of the $$\zeta$$s, that is $$\operatorname{Var}[S_k^{\zeta}] = \sum_{j=1}^k \sigma^2_{\zeta_j}$$.

In our examples, the single coin tosses $$x_i$$ comprising the sample are uncorrelated as they are independent, and, 
as they are identically distributed, all have same variance $$\sigma^2$$. 
Hence, the variance of their sum $$S_{n}^x:=\sum_{i=1}^n x_n$$ is $$n$$ times the variance of $$x$$ that is
$$\operatorname{Var}[S_n^x] = \sum_{i=1}^n \operatorname{Var}[x_i] = \sum_{i=1}^n \sigma^2 = n\sigma^2$$. 
The SE of $$\bar{x}_n$$ which is equal to its 
standard deviation is thus 

$$
SE[\bar{x}_n] 
= SE\left[\frac{S_n^x}{n}\right]  
= \frac{1}{n} SE[S_n^x] 
= \frac{1}{n}\sqrt{\operatorname{Var}[S_n^x]} = \frac{1}{n} \sqrt{\sigma^2 n} = \frac{\sigma}{\sqrt{n}}.
$$

which (leftmost and rightmost term) is the formula stated above.

### Tossing coins again



 ![png]({{ site.baseurl }}/img/2021-08-31-std/00.png)
    
 ![png]({{ site.baseurl }}/img/2021-08-31-std/01.png)

 ![png]({{ site.baseurl }}/img/2021-08-31-std/02.png)

 ![png]({{ site.baseurl }}/img/2021-08-31-std/03.png)

 ![png]({{ site.baseurl }}/img/2021-08-31-std/03a.png)

 ![png]({{ site.baseurl }}/img/2021-08-31-std/06.png)




### Bootstraping: Estimate the Unknown


 ![png]({{ site.baseurl }}/img/2021-08-31-std/04.png)

 ![png]({{ site.baseurl }}/img/2021-08-31-std/05.png)

 ![png]({{ site.baseurl }}/img/2021-08-31-std/05a.png)

 ![png]({{ site.baseurl }}/img/2021-08-31-std/07.png)

### Confidence Intervals: Augmenting the LLN with the SE

requires the normal approximation. perhaps for another post.


### The Catch Again
biases
bootstraping
convergence, high precision very diffucilt